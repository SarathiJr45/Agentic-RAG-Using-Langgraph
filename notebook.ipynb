{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je adore le programmation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def get_llama_llm(model: str = \"llama3-8b-8192\", temperature: float = 0.2):\n",
    "    return ChatGroq(\n",
    "        model=model,\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "llm = get_llama_llm()\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages).content\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend/data_ingestion/pdf_loader.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdfs(pdf_folder):\n",
    "\n",
    "    \n",
    "\n",
    "    pdf_paths = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "    page_texts = []\n",
    "\n",
    "    for path in pdf_paths:\n",
    "        reader = PdfReader(path)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                page_texts.append(text)\n",
    "    \n",
    "    return page_texts  # Now returning list of pages\n",
    "\n",
    "# backend/data_ingestion/pdf_loader.py (continued)\n",
    "\n",
    "def chunk_text(pages, max_tokens=300):\n",
    "    from textwrap import wrap\n",
    "\n",
    "    chunks = []\n",
    "    for page in pages:\n",
    "        chunks.extend(wrap(page, max_tokens))\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\AgenticRAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "def load_embedding_model(model_name=\"all-MiniLM-L6-v2\"):\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "def get_embeddings(model, texts):\n",
    "    return model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "def save_vector_store(embeddings, chunks, index_path, metadata_path):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "def load_vector_store(index_path, metadata_path):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(metadata_path, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:06<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# run_vector_pipeline.py\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "from data_ingestion.pdf_loader import extract_text_from_pdfs, chunk_text\n",
    "from data_ingestion.vector_store import load_embedding_model, get_embeddings\n",
    "from data_ingestion.vector_store import save_vector_store\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Define paths\n",
    "pdf_folder = \"docs\"\n",
    "index_path = \"faiss_store/faiss_index.index\"\n",
    "metadata_path = \"faiss_store/chunks.pkl\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(index_path), exist_ok=True)\n",
    "\n",
    "# Ingest and process PDFs\n",
    "text = extract_text_from_pdfs(pdf_folder)\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "# Generate embeddings\n",
    "model = load_embedding_model()\n",
    "embeddings = get_embeddings(model, chunks)\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# # Save vector store\n",
    "# save_vector_store(\n",
    "#     embeddings, chunks,\n",
    "#     index_path=index_path,\n",
    "#     metadata_path=metadata_path\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Vector store created and saved at:\")\n",
    "# print(f\"   ‚Üí Index: {index_path}\")\n",
    "# print(f\"   ‚Üí Metadata: {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je adore le programmation.\n",
      "\n",
      "(Note: \"Je\" is the first person singular pronoun meaning \"I\", \"adore\" is the verb meaning \"to love\", and \"le programmation\" is the noun phrase meaning \"programming\".)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Topic: Artificial Intelligence (AI) and Large Language Models (LLMs)\\n\\nRefined Query: Can you explain the working of AI agents and Large Language Models (LLMs), including their architecture, capabilities, and applications?\\n\\nBreakdown:\\n\\n* Topic: Artificial Intelligence (AI) and Large Language Models (LLMs)\\n* Refined Query: Explain the working of AI agents and LLMs, including their architecture, capabilities, and applications.\\n\\nThis refined query is more specific and focused, allowing me to provide a more detailed and accurate response.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# import os\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from llm.groq_llama import get_llama_llm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = get_llama_llm()\n",
    "\n",
    "# Define output structure\n",
    "\n",
    "\n",
    "# Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a planner agent in a RAG system. Given a user's question, Break it down in to a topic and a refined query.\"),\n",
    "    (\"user\", \"\"\"User query: {question}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# Chain = Prompt ‚Üí LLM ‚Üí JSON Parser\n",
    "chain = prompt | llm \n",
    "\n",
    "def plan_user_query(question: str) -> dict:\n",
    "    return chain.invoke({\"question\": question})\n",
    "  \n",
    "plan_user_query(\"explain llm and ai agents on its working\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context:\n",
      "\n",
      "as well as under General Condition N o c of this Policy.      l)  Subrogation   Insured and/or any Insured Persons shall at their o wn expense do or concur in doing or permit to be do ne  all such acts and things that may be necessary or r easonably required by Insurer for the purpose of en forcing\n",
      "\n",
      "found that there are  multiple policies obtained by the Insured covering   hospitalisation reimbursement benefit provided by t his policy and such information on other existing  hospitalisation reimbursement/health insurance poli cies is not declared/provided to us in the proposal   form, the policy\n",
      "\n",
      "reasonably a nd necessarily incurred by or on behalf of such Ins ured Person,  but not exceeding the sum Insured for the insured p erson as mentioned in the schedule of the policy. T he  following benefits are covered under this policy su bject to the sub-limits as stipulated in the policy\n"
     ]
    }
   ],
   "source": [
    "# agents/retriever_agent.py\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load FAISS index + chunk metadata\n",
    "def load_vector_store(\n",
    "    index_path=\"faiss_store/faiss_index.index\",\n",
    "    metadata_path=\"faiss_store/chunks.pkl\"\n",
    "):\n",
    "    if not os.path.exists(index_path) or not os.path.exists(metadata_path):\n",
    "        raise FileNotFoundError(\"FAISS index or chunk file not found\")\n",
    "\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(metadata_path, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    return index, chunks\n",
    "\n",
    "# Embed the query\n",
    "def embed_query(query: str, model: SentenceTransformer):\n",
    "    return np.array([model.encode(query)])\n",
    "\n",
    "# Perform retrieval\n",
    "def retrieve_context(query: str, index, chunks, embed_model, k: int = 3) -> str:\n",
    "    query_vec = embed_query(query, embed_model)\n",
    "    _, I = index.search(query_vec, k)\n",
    "    context = \"\\n\\n\".join([chunks[i] for i in I[0]])\n",
    "    return context\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    index, chunks = load_vector_store()\n",
    "\n",
    "    query = \"What are the policies available?\"\n",
    "    context = retrieve_context(query, index, chunks, model)\n",
    "\n",
    "    print(\"Retrieved Context:\\n\")\n",
    "    print(context[:1000])  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Content Only Result:\n",
      "\n",
      "Wimbledon 2024 live updates: Carlos Alcaraz defeats Novak Djokovic in straight sets to defend his crown In the 2023 final, Carlos Alcaraz won his first Wimbledon title, and only his second Grand Slam title, after beating Novak Djokovic in a five-set thriller on Centre Court. GO FURTHER Novak Djokovic and Carlos Alcaraz‚Äôs Wimbledon final is a duel of extraordinary quests Novak Djokovic set up a Wimbledon rematch with Carlos Alcaraz by beating Lorenzo Musetti, 6-4, 7-6, 6-3 on Centre Court on Friday, concluding his run to the final at the All England Club that started just 25 days after surgery on a torn meniscus in his right knee. GO FURTHER Novak Djokovic beats Lorenzo Musetti for Wimbledon final against Carlos Alcaraz\n",
      "\n",
      "\n",
      "Defending champion Carlos Alcaraz reached his fourth Grand Slam final at Wimbledon on Friday, UK time, when he recovered from a set down to defeat Daniil Medvedev. It will the second successive Wimbledon final of Alcaraz against Djokovic. Booed again, Djokovic plays the Wimbledon crowd like a fiddle in setting up Alcaraz rematch It was the third time at this year‚Äôs Wimbledon that Alcaraz had dropped the first set. Djokovic, 37, can equal Roger Federer‚Äôs record of eight Wimbledon titles and become the tournament‚Äôs oldest champion of the modern era if he avenges last year‚Äôs final loss to Alcaraz. Booed again, Djokovic plays the Wimbledon crowd like a fiddle in setting up Alcaraz rematch\n",
      "\n",
      "\n",
      "Novak Djokovic accepting of Carlos Alcaraz defeat: 'I don't think I could have done more' | ATP Tour | Tennis *   [News Releases](https://www.atptour.com/en/media/media-releases) *   [Newsletters](https://www.atptour.com/en/newsletters/subscribe?utm_source=atp&utm_medium=footer&utm_campaign=newsletter_link) *   [Challenger Media Guide](https://www.atptour.com/-/media/files/rankings-and-stats/atp-challenger-tour-media-guide.pdf) *   [Rankings and Info Reports](https://www.atptour.com/en/media/rankings-and-stats) *   [ATP Tour Archive](https://www.atptourarchive.com/) *   [Official ATP Tennis Club](https://www.atptour.com/en/partnerships/official-atp-tennis-club) *   [ATP Partners](https://www.atptour.com/en/partnerships/atp-partners) *   [ATP Media](https://www.atpmedia.tv/) *   [Tennis Radio](https://www.atptour.com/en/news/tennis-radio) Alcaraz gives thumbs up for injury recovery after Rome opener Ruud: ‚ÄòI always dreamed about winning tournaments like this‚Äô Alcaraz gives thumbs up for injury recovery after Rome opener Ruud: ‚ÄòI always dreamed about winning tournaments like this‚Äô Alcaraz gives thumbs up for injury recovery after Rome opener Ruud: ‚ÄòI always dreamed about winning tournaments like this‚Äô Alcaraz gives thumbs up for injury recovery after Rome opener Ruud: ‚ÄòI always dreamed about winning tournaments like this‚Äô\n",
      "\n",
      "\n",
      "Daniil Medvedev of Russia plays a forehand return to Jannik Sinner of Italy during their quarterfinal match at the Wimbledon tennis championships in London, Tuesday, July 9, 2024. Jannik Sinner of Italy plays a forehand return to Daniil Medvedev of Russia during their quarterfinal match at the Wimbledon tennis championships in London, Tuesday, July 9, 2024. Jannik Sinner of Italy plays a forehand return to Daniil Medvedev of Russia during their quarterfinal match at the Wimbledon tennis championships in London, Tuesday, July 9, 2024. Jannik Sinner of Italy plays a forehand return to Daniil Medvedev of Russia during their quarterfinal match at the Wimbledon tennis championships in London, Tuesday, July 9, 2024.\n",
      "\n",
      "\n",
      "Find out how you can save today!Senior Apartments | Search Ads | [Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=thumbs-feed-01-rd:Below%20Article%20Thumbnails%20|%20Card%202:)[Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=thumbs-feed-01-rd:Below%20Article%20Thumbnails%20|%20Card%202:) Learn More Shocking ED Unknown Tricks Doctors Want You To KnowTake a look you never knowEd Guide | Search Ads | [Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=thumbs-feed-01-a-rd:Below%20Article%20Thumbnails%20|%20Card%206:)[Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=thumbs-feed-01-a-rd:Below%20Article%20Thumbnails%20|%20Card%206:) Learn More New Electric Cars Come With Tiny Price Tags (Take A Look)FrequentSearches | Search Ads | [Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=thumbs-feed-01-rd:Below%20Article%20Thumbnails%20|%20Card%209:)[Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=thumbs-feed-01-rd:Below%20Article%20Thumbnails%20|%20Card%209:) Fifty and out: How Gen X became the biggest work losers (with 10 years still to go)As the heyday of the creative economy boom becomes a distant memory, Stephen Armstrong looks at the generation who were once the high-rollers and are now facing an uncertain futurePromoted by Independent Premium | [Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=native-thumbs-feed-01-rd:Below%20Article%20Thumbnails%20|%20Card%2011:)[Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=native-thumbs-feed-01-rd:Below%20Article%20Thumbnails%20|%20Card%2011:) New Small Electric Car For Seniors - The Price May Surprise YouFavoriteSearches | Search Ads | [Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=thumbs-feed-01-rd:Below%20Article%20Thumbnails%20|%20Card%2014:)[Sponsored](https://popup.taboola.com/en/?template=colorbox&utm_source=eslmedia-theindependent&utm_medium=referral&utm_content=thumbs-feed-01-rd:Below%20Article%20Thumbnails%20|%20Card%2014:)\n"
     ]
    }
   ],
   "source": [
    "# agents/websearch_agent.py\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Tavily tool with API key\n",
    "tavily_tool = TavilySearch(\n",
    "    api_key=os.getenv(\"TAVILY_API_KEY\"),\n",
    "    max_results=5,\n",
    "    topic=\"general\"\n",
    ")\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    result = tavily_tool.invoke({\"query\": query})\n",
    "    \n",
    "    # Extract only the content from each search result\n",
    "    content_only = []\n",
    "    \n",
    "    if 'results' in result:\n",
    "        for item in result['results']:\n",
    "            if 'content' in item and item['content']:\n",
    "                content_only.append(item['content'])\n",
    "    \n",
    "    # Join all content pieces with separators\n",
    "    return \"\\n\\n\\n\".join(content_only)\n",
    "\n",
    "# Test run\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"What happened at the last Wimbledon?\"\n",
    "    \n",
    "    print(\"üîç Content Only Result:\\n\")\n",
    "    print(web_search(query))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " According to the context, it is mentioned that \"found that there are multiple policies obtained by the Insured covering hospitalisation reimbursement benefit provided by this policy and such information on other existing hospitalisation reimbursement/health insurance policies is not declared/provided to us in the proposal form, the policy...\"\n",
      "\n",
      "Therefore, the answer is: There are multiple policies obtained by the Insured covering hospitalisation reimbursement benefit.\n"
     ]
    }
   ],
   "source": [
    "# agents/qa_agent.py\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "\n",
    "from llm.groq_llama import get_llama_llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load the LLaMA LLM (via langchain-groq)\n",
    "llm = get_llama_llm()\n",
    "\n",
    "# Define prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert research assistant. Always answer based on the provided context.\"),\n",
    "    (\"user\", \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer the question in a clear and concise manner, only using the context. If the answer isn't found, say \"The context does not contain that information.\" \"\"\")\n",
    "])\n",
    "\n",
    "# Chain: Prompt -> LLM\n",
    "qa_chain = prompt | llm\n",
    "\n",
    "# Main callable QA function\n",
    "def answer_from_context(context: str, question: str) -> str:\n",
    "    return qa_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    }).content\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from agents.retriver_agent import load_vector_store, retrieve_context\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    # Load retriever index & model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    index, chunks = load_vector_store()\n",
    "\n",
    "    # User query\n",
    "    question = \"What are the policies available?\"\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(question, index, chunks, model, k=4)\n",
    "\n",
    "    # Get answer from QA agent\n",
    "    answer = answer_from_context(context, question)\n",
    "    print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langgraph_app/graph.py\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "# from agents.planner_agent import refine_query\n",
    "# from agents.retriever_agent import load_vector_store, retrieve_context\n",
    "# from agents.websearch_agent import web_search\n",
    "# from agents.qa_agent import answer_from_context\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "index, chunks = load_vector_store()\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# --- Nodes ---\n",
    "def planner_node(state):\n",
    "    user_query = state[\"query\"]\n",
    "    \n",
    "    if isinstance(user_query, BaseMessage):\n",
    "        user_query = user_query.content\n",
    "        \n",
    "    refined = plan_user_query(user_query)\n",
    "    return {\"refined_query\": refined}\n",
    "\n",
    "def retriever_node(state):\n",
    "    query = state[\"refined_query\"]\n",
    "    \n",
    "    # üëá Convert BaseMessage to string\n",
    "    if isinstance(query, BaseMessage):\n",
    "        query = query.content\n",
    "        \n",
    "    context = retrieve_context(query, index, chunks, embed_model)\n",
    "    return {\"context\": context}\n",
    "\n",
    "def qa_node(state):\n",
    "    query = state[\"refined_query\"]\n",
    "    \n",
    "    if isinstance(query, BaseMessage):\n",
    "        query = query.content\n",
    "        \n",
    "    context = state.get(\"context\", \"\")\n",
    "    answer = answer_from_context(context, query)\n",
    "\n",
    "    if \"[The context does not contain that information.]\" in answer:  # Check trigger\n",
    "        return {\"needs_web\": True}\n",
    "    \n",
    "    return {\"answer\": answer, \"needs_web\": False}\n",
    "\n",
    "def websearch_node(state):\n",
    "    query = state[\"refined_query\"]\n",
    "    \n",
    "    if isinstance(query, BaseMessage):\n",
    "        query = query.content\n",
    "        \n",
    "    context = web_search(query)\n",
    "    return {\"context\": context}\n",
    "\n",
    "# --- Graph Schema ---\n",
    "\n",
    "\n",
    "class AgenticRAGState(TypedDict):\n",
    "    query: str\n",
    "    refined_query: str\n",
    "    context: str\n",
    "    answer: str\n",
    "    needs_web: bool\n",
    "\n",
    "builder = StateGraph(AgenticRAGState)\n",
    "\n",
    "\n",
    "builder.add_node(\"planner\", planner_node)\n",
    "builder.add_node(\"retriever\", retriever_node)\n",
    "builder.add_node(\"qa\", qa_node)\n",
    "builder.add_node(\"websearch\", websearch_node)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"retriever\")\n",
    "# builder.add_edge(\"retriever\",\"websearch\")\n",
    "builder.add_edge(\"retriever\", \"qa\")\n",
    "\n",
    "# Conditional edge: fallback to web if QA fails\n",
    "builder.add_conditional_edges(\"qa\", lambda state: state[\"needs_web\"], {\n",
    "    True: \"websearch\",\n",
    "    False: END\n",
    "})\n",
    "\n",
    "# Websearch leads to 2nd QA pass\n",
    "builder.add_edge(\"websearch\", \"qa\")\n",
    "\n",
    "builder.set_finish_point(\"qa\")\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# # from langgraph_app.graph import graph\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# while True:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#     result = graph.invoke({\"query\": q})\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#     print(\"\\n‚úÖ Answer:\\n\", result.get(\"answer\", \"No answer\"))\u001b[39;00m\n\u001b[32m     10\u001b[39m q = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚ùì Ask me something (or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m result = \u001b[43mgraph\u001b[49m.invoke({\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: q})\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Answer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, result.get(\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNo answer\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph_app.graph import graph\n",
    "\n",
    "while True:\n",
    "    q = input(\"\\n‚ùì Ask me something (or 'exit'): \")\n",
    "    if q.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    result = graph.invoke({\"query\": q})\n",
    "    print(\"\\n‚úÖ Answer:\\n\", result.get(\"answer\", \"No answer\"))\n",
    "\n",
    "# q = input(\"\\n‚ùì Ask me something (or 'exit'): \")\n",
    "\n",
    "# result = graph.invoke({\"query\": q})\n",
    "# print(\"\\n‚úÖ Answer:\\n\", result.get(\"answer\", \"No answer\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
